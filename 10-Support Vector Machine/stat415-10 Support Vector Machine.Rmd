---
title: "Stat415-homework10"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This question uses the same crabs data used in Homework 9. Use the following code to split the data into training and test sets:
```{r}
set.seed(45678)
library(MASS)
data(crabs)
attach(crabs)
blueMale = which(sp == "B" & sex == "M")
orangeMale = which(sp == "O" & sex == "M")
blueFemale = which(sp == "B" & sex == "F")
orangeFemale = which(sp == "O" & sex == "F")
train_id = c(sample(blueMale, size = trunc(0.80 * length(blueMale))),
sample(orangeMale, size = trunc(0.80 * length(orangeMale))),
sample(blueFemale, size = trunc(0.80 * length(blueFemale))),
sample(orangeFemale, size = trunc(0.80 * length(orangeFemale))))
crabs_train = crabs[train_id, ]
crabs_test = crabs[-train_id,]
```
(a) Fit a linear support vector machine to the data with various values of cost, in order to predict Species from the five numerical
measurements. Omit the variable Sex for this homework. Report the cross-validation errors associated with different values of cost.
Comment on your results and make some relevant plots.
```{r}
# omit the variable Sex
data_delSex = crabs[,-2]
# omit the variable index
data_delSex = data_delSex[,-2]
testdat = data_delSex[-train_id,]
traindat = data_delSex[train_id,]
# support vector machine
library(e1071)
svmfit = svm(sp ~., data = traindat, kernel = "linear", cost = 0.001, scale = FALSE)
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "linear", cost = 0.1, scale = FALSE)
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "linear", cost = 100, scale = FALSE)
summary(svmfit)
# Make some relevant plots
par(mfrow=c(2,2))
dat = data.frame(x = traindat[,-1], y = as.factor(traindat$sp))
plot(dat[,1:2],col=data_delSex$sp)
plot(dat[,2:3],col=data_delSex$sp)
plot(dat[,2],dat[,4],col=data_delSex$sp)
plot(dat[,2],dat[,5],col=data_delSex$sp)
# CV errors
set.seed(45678)
tune.out = tune(svm, sp ~ ., data = data_delSex[train_id,], kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
ypred = predict(bestmod, testdat)
table(predict = ypred, truth = testdat$sp)
```
Comment: According to the plot, we can notice that the linear support vector machine should be effective in this data set since the data could be divided by a line visually. Linear support vector machine with cost=0.001,0.1 and 100 has been created.Cross-validation errors associated with different values of cost have also been shown above. when cost=1e+00, the error is the smallest, zero and the corresponding gamma is 0.2. After using the best model to predict in the test data, all the predictions are correct. 


(b) Fit nonlinear SVMs with radial and polynomial kernels, with different values of gamma and degree and cost. Report the cross-validation errors associated with different values of cost. Comment on your results and make some relevant plots.
```{r}
set.seed(45678)
svmfit = svm(sp ~., data = traindat, kernel = "radial", ranges = list(cost = 1, gamma = 1))
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "radial", cost = 0.1, ranges = list(cost = 10, gamma = 1))
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "radial", cost = 100, ranges = list(cost = 1, gamma = 4))
summary(svmfit)
tune.out = tune(svm, sp ~ ., data = traindat, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 40), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
table(true = testdat$sp,pred = predict(bestmod, newdata = testdat))
```
Comment: The error plot with different gamma and cost value has been shown above.Non-linear support vector machine with cost=0.1, 1, 10 and 20 has been created. Cross-validation errors associated with different values of cost and gamma have also been shown above. when cost=10 and gamma=0.5, we have the best model for radial kernels since the error is the smallest, 0.01250.After using the best model to predict in the test data, all the predictions of radial kernels are correct. 

```{r}
set.seed(45678)
svmfit = svm(sp ~., data = traindat, kernel = "polynomial", ranges = list(cost = 1, gamma = 1, degree = 3))
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "polynomial", cost = 0.1, ranges = list(cost = 10, gamma = 1, degree = 3))
summary(svmfit)
svmfit = svm(sp ~., data = traindat, kernel = "polynomial", cost = 100, ranges = list(cost = 1, gamma = 4, degree = 4))
summary(svmfit)
tune.out2 = tune(svm, sp ~ ., data = traindat, kernel = "polynomial", ranges = list(cost = c(0.1, 1, 10), gamma = c(0.5, 1, 2), degree = c(1, 2, 3, 4)))
summary(tune.out2)
bestmod2 = tune.out2$best.model
summary(bestmod2)
table(true = testdat$sp,pred = predict(bestmod2, newdata = testdat))
```


```{r}
with(tune.out$performances, {
plot(error[gamma == 0.5] ~ cost[gamma == 0.5], ylim = c(0.01, 0.35), type = "o", col = rainbow(5)[1], ylab = "CV error", xlab = "cost")
lines(error[gamma == 1] ~ cost[gamma == 1], type = "o", col = rainbow(5)[2])
lines(error[gamma == 2] ~ cost[gamma == 2], type = "o", col = rainbow(5)[3])
lines(error[gamma == 3] ~ cost[gamma == 3], type = "o", col = rainbow(5)[4])
lines(error[gamma == 4] ~ cost[gamma == 4], type = "o", col = rainbow(5)[5])
})
legend("top", horiz = T, legend = c(0.5, 1:4), col = rainbow(5), lty = 1, cex = .75, title = "gamma")
```
```{r}
par(mfrow=c(1,3))
with(tune.out2$performances, {
plot(error[degree == 1][gamma == 0.5] ~ cost[degree == 1][gamma == 0.5], ylim = c(0.01, 1), type = "o", col = rainbow(3)[1], ylab = "CV error", xlab = "cost")
lines(error[degree == 1][gamma == 1] ~ cost[degree == 1][gamma == 1], type = "o", col = rainbow(3)[2])
lines(error[degree == 1][gamma == 2] ~ cost[degree == 1][gamma == 2], type = "o", col = rainbow(3)[3])
})
legend("top", horiz = T, legend = c(0.5, 1:2), col = rainbow(3), lty = 1, cex = .75, title = "gamma")

with(tune.out2$performances, {
plot(error[degree == 2][gamma == 0.5] ~ cost[degree == 2][gamma == 0.5], ylim = c(0.01, 1), type = "o", col = rainbow(3)[1], ylab = "CV error", xlab = "cost")
lines(error[degree == 2][gamma == 1] ~ cost[degree == 2][gamma == 1], type = "o", col = rainbow(3)[2])
lines(error[degree == 2][gamma == 2] ~ cost[degree == 2][gamma == 2], type = "o", col = rainbow(3)[3])
})
legend("top", horiz = T, legend = c(0.5, 1:2), col = rainbow(3), lty = 1, cex = .75, title = "gamma")

with(tune.out2$performances, {
plot(error[degree == 3][gamma == 0.5] ~ cost[degree == 3][gamma == 0.5], ylim = c(0.01, 1), type = "o", col = rainbow(3)[1], ylab = "CV error", xlab = "cost")
lines(error[degree == 3][gamma == 1] ~ cost[degree == 3][gamma == 1], type = "o", col = rainbow(3)[2])
lines(error[degree == 3][gamma == 2] ~ cost[degree == 3][gamma == 2], type = "o", col = rainbow(3)[3])
})
legend("top", horiz = T, legend = c(0.5, 1:2), col = rainbow(3), lty = 1, cex = .75, title = "gamma")
```
Comment: The error plot with different gamma and cost value has been shown above.Non-linear support vector machine with cost=0.1, 1, 10 has been created. Cross-validation errors associated with different values of cost and gamma and degree have also been shown above. when cost=1, degree = 1 and gamma=0.5, we have the best model for polynomial kernels. After using the best model to predict in the test data, all the predictions are correct.
